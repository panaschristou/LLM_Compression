{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#import sys\n",
    "#sys.path.append('/scratch/xc1490/projects/tmp/python_packages')\n",
    "#sys.path.append('/scratch/xc1490/projects/tmp/python_packages/') \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description='Tuning Pruned LLaMA (huggingface version)')\n",
    "\n",
    "    parser.add_argument('--base_model', type=str, default=\"decapoda-research/llama-7b-hf\", help='base model name')\n",
    "    parser.add_argument('--model_type', type=str, required=True, help = 'choose from ')\n",
    "    parser.add_argument('--ckpt', type=str, default=None)\n",
    "    parser.add_argument('--lora_ckpt', type=str, default=None)\n",
    "    parser.add_argument('--share_gradio', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.base_model = 'meta-llama/llama-7b-hf'\n",
    "        self.model_type = 'tune_prune_LLM'\n",
    "        self.ckpt = '/scratch/pc2442/Project/LLM-Pruner-main/tune_log/llama_0.2/checkpoint-1400'\n",
    "        self.lora_ckpt = '/scratch/pc2442/Project/LLM-Pruner-main/tune_log/llama_0.2/checkpoint-1400'\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install --target=/home/xc1490/home/projects/hpml/python_packages gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mDEPRECATION: Loading egg at /ext3/miniconda3/lib/python3.11/site-packages/bitsandbytes-0.41.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Loading egg at /ext3/miniconda3/lib/python3.11/site-packages/bitsandbytes-0.41.3-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch>=1.7.1 in /ext3/miniconda3/lib/python3.11/site-packages (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: transformers>=4.28.1 in /ext3/miniconda3/lib/python3.11/site-packages (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (4.31.0)\n",
      "Requirement already satisfied: datasets in /ext3/miniconda3/lib/python3.11/site-packages (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (2.14.6)\n",
      "Requirement already satisfied: sentencepiece in /ext3/miniconda3/lib/python3.11/site-packages (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 4)) (0.1.99)\n",
      "Requirement already satisfied: accelerate in /ext3/miniconda3/lib/python3.11/site-packages (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 5)) (0.21.0)\n",
      "Requirement already satisfied: wandb in /ext3/miniconda3/lib/python3.11/site-packages (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (0.15.3)\n",
      "Collecting ptflops (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 7))\n",
      "  Using cached ptflops-0.7.1.2-py3-none-any.whl\n",
      "Collecting gradio (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached gradio-4.10.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting rouge-score>=0.0.4 (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 9))\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Collecting jsonlines (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 10))\n",
      "  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting numexpr (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 11))\n",
      "  Using cached numexpr-2.8.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Collecting sacrebleu==1.5.0 (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 12))\n",
      "  Using cached sacrebleu-1.5.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.24.1 in /ext3/miniconda3/lib/python3.11/site-packages (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 13)) (1.2.2)\n",
      "Collecting sqlitedict (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 14))\n",
      "  Using cached sqlitedict-2.1.0-py3-none-any.whl\n",
      "Collecting importlib-resources (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 16))\n",
      "  Using cached importlib_resources-6.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pycountry (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 17))\n",
      "  Using cached pycountry-23.12.11-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting omegaconf (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 18))\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Collecting pytablewriter (from -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 19))\n",
      "  Using cached pytablewriter-1.2.0-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting portalocker (from sacrebleu==1.5.0->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 12))\n",
      "  Using cached portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: filelock in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: sympy in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /ext3/miniconda3/lib/python3.11/site-packages (from torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /ext3/miniconda3/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (68.0.0)\n",
      "Requirement already satisfied: wheel in /ext3/miniconda3/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (0.41.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cmake in /ext3/miniconda3/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (3.27.7)\n",
      "Requirement already satisfied: lit in /ext3/miniconda3/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (17.0.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /ext3/miniconda3/lib/python3.11/site-packages (from transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /ext3/miniconda3/lib/python3.11/site-packages (from transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /ext3/miniconda3/lib/python3.11/site-packages (from transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /ext3/miniconda3/lib/python3.11/site-packages (from transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /ext3/miniconda3/lib/python3.11/site-packages (from transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (2023.10.3)\n",
      "Requirement already satisfied: requests in /ext3/miniconda3/lib/python3.11/site-packages (from transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /ext3/miniconda3/lib/python3.11/site-packages (from transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /ext3/miniconda3/lib/python3.11/site-packages (from transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /ext3/miniconda3/lib/python3.11/site-packages (from transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /ext3/miniconda3/lib/python3.11/site-packages (from datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (14.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /ext3/miniconda3/lib/python3.11/site-packages (from datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /ext3/miniconda3/lib/python3.11/site-packages (from datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /ext3/miniconda3/lib/python3.11/site-packages (from datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /ext3/miniconda3/lib/python3.11/site-packages (from datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /ext3/miniconda3/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /ext3/miniconda3/lib/python3.11/site-packages (from datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (3.9.1)\n",
      "Requirement already satisfied: psutil in /ext3/miniconda3/lib/python3.11/site-packages (from accelerate->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 5)) (5.9.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /ext3/miniconda3/lib/python3.11/site-packages (from wandb->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /ext3/miniconda3/lib/python3.11/site-packages (from wandb->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (3.1.40)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /ext3/miniconda3/lib/python3.11/site-packages (from wandb->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (1.38.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /ext3/miniconda3/lib/python3.11/site-packages (from wandb->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /ext3/miniconda3/lib/python3.11/site-packages (from wandb->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /ext3/miniconda3/lib/python3.11/site-packages (from wandb->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /ext3/miniconda3/lib/python3.11/site-packages (from wandb->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /ext3/miniconda3/lib/python3.11/site-packages (from wandb->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (4.25.1)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting fastapi (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached fastapi-0.105.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ffmpy (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached ffmpy-0.3.1-py3-none-any.whl\n",
      "Collecting gradio-client==0.7.3 (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached gradio_client-0.7.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting httpx (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached httpx-0.25.2-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2))\n",
      "  Using cached huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /ext3/miniconda3/lib/python3.11/site-packages (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /ext3/miniconda3/lib/python3.11/site-packages (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (3.8.2)\n",
      "Collecting orjson~=3.0 (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached orjson-3.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /ext3/miniconda3/lib/python3.11/site-packages (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (10.1.0)\n",
      "Collecting pydantic>=2.0 (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "Collecting pydub (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached python_multipart-0.0.6-py3-none-any.whl (45 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting semantic-version~=2.0 (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting typer<1.0,>=0.9 (from typer[all]<1.0,>=0.9->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting uvicorn>=0.14.0 (from gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==0.7.3->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "Collecting absl-py (from rouge-score>=0.0.4->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 9))\n",
      "  Using cached absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting nltk (from rouge-score>=0.0.4->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 9))\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: six>=1.14.0 in /ext3/miniconda3/lib/python3.11/site-packages (from rouge-score>=0.0.4->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 9)) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /ext3/miniconda3/lib/python3.11/site-packages (from jsonlines->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 10)) (23.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /ext3/miniconda3/lib/python3.11/site-packages (from scikit-learn>=0.24.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 13)) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /ext3/miniconda3/lib/python3.11/site-packages (from scikit-learn>=0.24.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 13)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /ext3/miniconda3/lib/python3.11/site-packages (from scikit-learn>=0.24.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 13)) (3.2.0)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 18))\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Collecting DataProperty<2,>=1.0.1 (from pytablewriter->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 19))\n",
      "  Using cached DataProperty-1.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mbstrdecoder<2,>=1.0.0 (from pytablewriter->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 19))\n",
      "  Using cached mbstrdecoder-1.1.3-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0 (from pytablewriter->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 19))\n",
      "  Using cached pathvalidate-3.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting tabledata<2,>=1.3.1 (from pytablewriter->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 19))\n",
      "  Using cached tabledata-1.3.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5 (from pytablewriter->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 19))\n",
      "  Using cached tcolorpy-0.1.4-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting typepy<2,>=1.3.2 (from typepy[datetime]<2,>=1.3.2->pytablewriter->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 19))\n",
      "  Using cached typepy-1.3.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /ext3/miniconda3/lib/python3.11/site-packages (from altair<6.0,>=4.2.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (4.19.2)\n",
      "Collecting toolz (from altair<6.0,>=4.2.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /ext3/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /ext3/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /ext3/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /ext3/miniconda3/lib/python3.11/site-packages (from aiohttp->datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /ext3/miniconda3/lib/python3.11/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (4.0.11)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /ext3/miniconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /ext3/miniconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /ext3/miniconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /ext3/miniconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /ext3/miniconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /ext3/miniconda3/lib/python3.11/site-packages (from matplotlib~=3.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (2.8.2)\n",
      "Collecting chardet<6,>=3.0.4 (from mbstrdecoder<2,>=1.0.0->pytablewriter->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 19))\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /ext3/miniconda3/lib/python3.11/site-packages (from pandas->datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /ext3/miniconda3/lib/python3.11/site-packages (from pandas->datasets->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 3)) (2023.3)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.5 (from pydantic>=2.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached pydantic_core-2.14.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /ext3/miniconda3/lib/python3.11/site-packages (from requests->transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ext3/miniconda3/lib/python3.11/site-packages (from requests->transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /ext3/miniconda3/lib/python3.11/site-packages (from requests->transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ext3/miniconda3/lib/python3.11/site-packages (from requests->transformers>=4.28.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 2)) (2023.11.17)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich<14.0.0,>=10.11.0 (from typer[all]<1.0,>=0.9->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Collecting anyio<4.0.0,>=3.7.1 (from fastapi->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting httpcore==1.* (from httpx->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in /ext3/miniconda3/lib/python3.11/site-packages (from httpx->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /ext3/miniconda3/lib/python3.11/site-packages (from sympy->torch>=1.7.1->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /ext3/miniconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /ext3/miniconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /ext3/miniconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /ext3/miniconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (0.10.6)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /ext3/miniconda3/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8)) (2.15.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio->-r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt (line 8))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached gradio-4.10.0-py3-none-any.whl (16.6 MB)\n",
      "Using cached gradio_client-0.7.3-py3-none-any.whl (304 kB)\n",
      "Using cached tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Using cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Using cached numexpr-2.8.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (377 kB)\n",
      "Using cached importlib_resources-6.1.1-py3-none-any.whl (33 kB)\n",
      "Using cached pycountry-23.12.11-py3-none-any.whl (6.2 MB)\n",
      "Using cached pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
      "Using cached aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "Using cached DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
      "Using cached huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "Using cached mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
      "Using cached orjson-3.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "Using cached pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
      "Using cached pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "Using cached pydantic_core-2.14.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "Using cached tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
      "Using cached tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\n",
      "Using cached typepy-1.3.2-py3-none-any.whl (31 kB)\n",
      "Using cached uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "Using cached absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "Using cached fastapi-0.105.0-py3-none-any.whl (93 kB)\n",
      "Using cached httpx-0.25.2-py3-none-any.whl (74 kB)\n",
      "Using cached httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "Using cached portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Installing collected packages: sqlitedict, pydub, ffmpy, antlr4-python3-runtime, websockets, typer, toolz, tomlkit, tcolorpy, shellingham, semantic-version, python-multipart, pydantic-core, pycountry, portalocker, pathvalidate, orjson, omegaconf, numexpr, nltk, mdurl, jsonlines, importlib-resources, h11, colorama, chardet, anyio, annotated-types, aiofiles, absl-py, uvicorn, starlette, sacrebleu, rouge-score, pydantic, mbstrdecoder, markdown-it-py, huggingface-hub, httpcore, typepy, rich, httpx, fastapi, gradio-client, altair, gradio, DataProperty, tabledata, pytablewriter, ptflops\n",
      "\u001b[33m  WARNING: The script nltk is installed in '/home/xc1490/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script chardetect is installed in '/home/xc1490/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script uvicorn is installed in '/home/xc1490/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script sacrebleu is installed in '/home/xc1490/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script markdown-it is installed in '/home/xc1490/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/xc1490/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script httpx is installed in '/home/xc1490/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not install packages due to an OSError: [Errno 122] Disk quota exceeded: '/home/xc1490/.local/lib/python3.11/site-packages/altair-5.2.0.dist-info/INSTALLER_fcg6uvw.tmp'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r /home/xc1490/home/projects/hpml/project/LLM-Pruner/requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import gradio as gr\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import GenerationConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from LLMPruner.peft import PeftModel\n",
    "\n",
    "#from utils.callbacks import Iteratorize, Stream\n",
    "#from utils.prompter import Prompter\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "torch_version = int(torch.__version__.split('.')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "\n",
    "if args.model_type == 'pretrain':\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.base_model,\n",
    "        low_cpu_mem_usage=True if torch_version >=9 else False\n",
    "    )\n",
    "    description = \"Model Type: {}\\n Base Model: {}\".format(args.model_type, args.base_model)\n",
    "elif args.model_type == 'pruneLLM':\n",
    "    pruned_dict = torch.load(args.ckpt, map_location='cpu')\n",
    "    tokenizer, model = pruned_dict['tokenizer'], pruned_dict['model']\n",
    "    description = \"Model Type: {}\\n Pruned Model: {}\".format(args.model_type, args.ckpt)\n",
    "elif args.model_type == 'tune_prune_LLM':\n",
    "    pruned_dict = torch.load(args.ckpt, map_location='cpu')\n",
    "    tokenizer, model = pruned_dict['tokenizer'], pruned_dict['model']\n",
    "    model = PeftModel.from_pretrained(\n",
    "        model,\n",
    "        args.lora_ckpt,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    description = \"Model Type: {}\\n Pruned Model: {}\\n LORA ckpt: {}\".format(args.model_type, args.ckpt, args.lora_ckpt)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if device == \"cuda\":\n",
    "    model.half()\n",
    "    model = model.cuda()\n",
    "\n",
    "# unwind broken decapoda-research config\n",
    "model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "model.config.bos_token_id = 1\n",
    "model.config.eos_token_id = 2\n",
    "\n",
    "model.eval()\n",
    "\n",
    "def evaluate(\n",
    "    input=None,\n",
    "    temperature=0.1,\n",
    "    top_p=0.75,\n",
    "    top_k=40,\n",
    "    max_new_tokens=128,\n",
    "    stream_output=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    inputs = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=top_p,\n",
    "            temperature=temperature,\n",
    "            max_length=max_new_tokens,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "    yield output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(input = 'how are you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0a1fcfa3f86426fad89dd6f0ba997c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Type your message here...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e74b2c4f7fbd45ab9a3bbb4fa4096550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Send', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f88328e902a4e21a03b283eb864ea42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Chatbot logic\n",
    "def evaluate(input):\n",
    "    \n",
    "    if \"hello\" in  input:\n",
    "        return \"Hello there! How can I assist you?\"\n",
    "    elif \"how are you\" in  input:\n",
    "        return \"I'm a bot, so I don't have feelings, but thanks for asking!\"\n",
    "    elif \"bye\" in  input:\n",
    "        return \"Goodbye! Have a great day!\"\n",
    "    else:\n",
    "        return \"I'm not sure how to respond to that. Can you ask something else?\"\n",
    "\n",
    "# Widgets\n",
    "text_input = widgets.Text(placeholder='Type your message here...')\n",
    "text_output = widgets.Output()\n",
    "button_send = widgets.Button(description='Send')\n",
    "\n",
    "def on_send_button_clicked(b):\n",
    "    with text_output:\n",
    "        clear_output()\n",
    "        print(f\"You: {text_input.value}\")\n",
    "        response = evaluate(text_input.value)\n",
    "        print(f\"Bot: {response}\")\n",
    "    text_input.value = ''  # Clear the input box\n",
    "\n",
    "button_send.on_click(on_send_button_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(text_input, button_send, text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gr.Interface(\n",
    "    fn=evaluate,\n",
    "    inputs=[\n",
    "        gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n",
    "        gr.components.Slider(\n",
    "            minimum=0, maximum=1, value=1, label=\"Temperature\"\n",
    "        ),\n",
    "        gr.components.Slider(\n",
    "            minimum=0, maximum=1, value=0.95, label=\"Top p\"\n",
    "        ),\n",
    "        gr.components.Slider(\n",
    "            minimum=0, maximum=100, step=1, value=50, label=\"Top k\"\n",
    "        ),\n",
    "        gr.components.Slider(\n",
    "            minimum=1, maximum=2000, step=1, value=128, label=\"Max tokens\"\n",
    "        ),\n",
    "        gr.components.Checkbox(label=\"Stream output\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.inputs.Textbox(\n",
    "            lines=5,\n",
    "            label=\"Output\",\n",
    "        )\n",
    "    ],\n",
    "    title=\"Evaluate Pruned Model\",\n",
    "    description=description,\n",
    ").queue().launch()#(server_name=\"0.0.0.0\", share=args.share_gradio)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
