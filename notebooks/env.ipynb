{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "singularity exec --nv --overlay /home/xc1490/home/apps/llm.ext3:rw /scratch/work/public/singularity/cuda11.0-cudnn8-devel-ubuntu18.04.sif /bin/bash\n",
    "source /ext3/env.sh\n",
    "\n",
    "srun -t0:30:00 --mem=12000 --gres=gpu:1 --pty /bin/bash\n",
    "\n",
    "cd /scratch/xc1490/projects/hpml/project\n",
    "\n",
    "https://github.com/TimDettmers/bitsandbytes\n",
    "    \n",
    "```\n",
    "git clone https://github.com/timdettmers/bitsandbytes.git\n",
    "\n",
    "singularity exec --nv --overlay /home/xc1490/home/apps/llm.ext3:rw /scratch/work/public/singularity/cuda11.0-cudnn8-devel-ubuntu18.04.sif /bin/bash\n",
    "source /ext3/env.sh\n",
    "\n",
    "module purge\n",
    "module load cudnn/8.6.0.163-cuda11\n",
    "module load cuda/11.3.1\n",
    "\n",
    "srun --cpus-per-task=1 --time=0:30:00 --mem=24000 --gres=gpu:1 --pty /bin/bash\n",
    "\n",
    "cd /scratch/xc1490/projects/hpml/project/bitsandbytes\n",
    "\n",
    "#CUDA_VERSION=113 make cuda11x\n",
    "python -m bitsandbytes\n",
    "\n",
    "#export BNB_CUDA_VERSION=110\n",
    "#export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, argparse, sys, os, errno\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sns.set()\n",
    "#sns.set_style('whitegrid')\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import scipy\n",
    "import sklearn\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append('/scratch/xc1490/projects/tmp/python_packages')\n",
    "sys.path.append('/scratch/xc1490/projects/tmp/python_packages/') #pip install --target=/home/xc1490/home/projects/tmp/python_packages package_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bitsandbytes 8 Bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/xc1490/projects/hpml/project/bitsandbytes\n"
     ]
    }
   ],
   "source": [
    "cd /scratch/xc1490/projects/hpml/project/bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75824627712"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.mem_get_info()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'68GB'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d1d40ba6e5422fa1ea29afceebb276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "'daryl149/llama-2-7b-chat-hf',\n",
    "device_map='auto',\n",
    "load_in_8bit=True,\n",
    "max_memory=max_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: 76000000000}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i: max_memory for i in range(n_gpus)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a1d06209d840d5b37a028c4d1accfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a988c5c4c804b2b8128383d0a1beff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.7/site-packages/transformers/generation/utils.py:1459: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamburg is in which country?\n",
      "\n",
      "Hamburg is a city located in the northern part of Germany. It is the second-largest city in Germany and the largest city in the northern region of Hamburg. The city is situated on the Elbe River and is known for its rich history, cultural attractions, and vibrant nightlife.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/TimDettmers/bitsandbytes\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MAX_NEW_TOKENS = 128\n",
    "model_name = 'decapoda-research/llama-7b-hf'\n",
    "model_name =  'daryl149/llama-2-7b-chat-hf'\n",
    "text = 'Hamburg is in which country?\\n'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "free_in_GB = int(torch.cuda.mem_get_info()[0]/1024**3)\n",
    "max_memory = f'{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB'\n",
    "\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  device_map='auto',\n",
    "  load_in_8bit=True,\n",
    "  max_memory=max_memory\n",
    ")\n",
    "generated_ids = model.generate(input_ids, max_length=MAX_NEW_TOKENS)\n",
    "print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
